{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Connect drive, download dataset"
      ],
      "metadata": {
        "id": "pDTiTPeUVjU4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QJk9bE-d2jK",
        "outputId": "6edba961-1d44-4761-f5e5-41f82270330f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "project_root_path = Path('drive/MyDrive/Colab Notebooks/ddpm')\n",
        "\n",
        "ds_path = Path('datasets')\n",
        "if not ds_path.exists():\n",
        "  # copy cats.zip to local\n",
        "  ds_path.mkdir(exist_ok=True, parents=True)\n",
        "  shutil.copy(project_root_path / Path('cats.zip'), ds_path)\n",
        "\n",
        "  # extract zip\n",
        "  with zipfile.ZipFile(ds_path / Path('cats.zip'), 'r') as zip_ref:\n",
        "      zip_ref.extractall(ds_path)\n",
        "\n",
        "  DATASET_PATH = ds_path / Path('cats')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check GPU configuration"
      ],
      "metadata": {
        "id": "j8h2KbOfVefi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4DWPQ0HbyJF",
        "outputId": "0dfa569b-3288-4063-b700-42fca1ee291c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Apr  1 04:25:09 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "# Check env settings\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download sources from github"
      ],
      "metadata": {
        "id": "uCGKKku1VOou"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWl-7sQib5wf",
        "outputId": "be7d909c-9a23-488c-c327-6946f249a97c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ddpm'...\n",
            "remote: Enumerating objects: 367, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 367 (delta 29), reused 35 (delta 16), pack-reused 310\u001b[K\n",
            "Receiving objects: 100% (367/367), 65.52 KiB | 729.00 KiB/s, done.\n",
            "Resolving deltas: 100% (241/241), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf ddpm\n",
        "!git clone https://github.com/mryanivtal/ddpm.git\n",
        "MAIN_DIR = './ddpm/src/ddpm_main.py'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train from scratch"
      ],
      "metadata": {
        "id": "JKcj2wMBVTAz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PPXOc4uhiK6m"
      },
      "outputs": [],
      "source": [
        "# !cd ddpm/src; python ddpm_main.py --datadir=\"../../datasets/cats\" --outdir=\"../../drive/MyDrive/Colab Notebooks/ddpm/output\" --timesteps=300 --batchsize=64 --randomseed=999 --dlworkers=2 --epochs=100 --onebatchperepoch=0 --checkpointevery=10 --inferonly=0 "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train - start from checkpoint"
      ],
      "metadata": {
        "id": "PxwjPEyoVWV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ddpm/src; python ddpm_main.py --datadir=\"../../datasets/cats\" --outdir=\"../../drive/MyDrive/Colab Notebooks/ddpm/output\" --timesteps=300 --batchsize=90 --lr=0.001 --randomseed=999 --dlworkers=2 --epochs=100 --checkpointevery=5 --modelcheckpoint=\"../../drive/MyDrive/Colab Notebooks/ddpm/output/saved/model_epoch_35.pt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX8QzMJCyrQv",
        "outputId": "ff2b2caa-0746-4c82-b1d8-20ea00f9c276"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n",
            "Mode: Train\n",
            "Output path: /content/ddpm/src/../../drive/MyDrive/Colab Notebooks/ddpm/output\n",
            "Random seed: 999\n",
            "GEN_LEARNING_RATE = 0.001\n",
            "NUM_EPOCHS = 100\n",
            "BATCH_SIZE = 90\n",
            "DL_WORKERS = 2\n",
            "Figure(1500x1500)\n",
            "Figure(1040x260)\n",
            "Epoch: 1  loss: 0.14752764374017716\n",
            "Figure(1500x1500)\n",
            "Figure(1040x260)\n",
            "Epoch: 2 Traceback (most recent call last):\n",
            "  File \"/content/ddpm/src/ddpm_main.py\", line 112, in <module>\n",
            "    batch_loss = train_batch(data, TIMESTEPS, model, noise_scheduler, optimizer, device)\n",
            "  File \"/content/ddpm/src/ddpm_functions.py\", line 70, in train_batch\n",
            "    optimizer.step()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\", line 140, in wrapper\n",
            "    out = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\", line 23, in _use_grad\n",
            "    ret = func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\", line 234, in step\n",
            "    adam(params_with_grad,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\", line 300, in adam\n",
            "    func(params,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\", line 410, in _single_tensor_adam\n",
            "    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference only from model checkpoint"
      ],
      "metadata": {
        "id": "7fGvFP4Ee_zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ddpm/src; python ddpm_main.py --datadir=\"../../datasets/cats\" --outdir=\"../../drive/MyDrive/Colab Notebooks/ddpm/output\" --timesteps=300 --inferonly=1  --modelcheckpoint=\"../../drive/MyDrive/Colab Notebooks/ddpm/output/saved/model_epoch_35.pt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et1FPoyCd3zh",
        "outputId": "59e4ac3a-0563-4bd0-f374-bff1431a3ed7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n",
            "Mode: Inference\n",
            "Output path: /content/ddpm/src/../../drive/MyDrive/Colab Notebooks/ddpm/output\n",
            "Random seed: 123\n",
            "GEN_LEARNING_RATE = 0.001\n",
            "NUM_EPOCHS = 100\n",
            "BATCH_SIZE = 50\n",
            "DL_WORKERS = 0\n",
            "Figure(1500x1500)\n",
            "Figure(1040x260)\n",
            "Figure(1500x1500)\n",
            "Figure(1040x260)\n",
            "Figure(1500x1500)\n",
            "Figure(1040x260)\n",
            "Figure(1500x1500)\n",
            "Figure(1040x260)\n",
            "Figure(1500x1500)\n",
            "Figure(1040x260)\n",
            "Figure(1500x1500)\n",
            "Figure(1040x260)\n",
            "Figure(1500x1500)\n",
            "Figure(1040x260)\n",
            "Figure(1500x1500)\n",
            "Figure(1040x260)\n",
            "Figure(1500x1500)\n",
            "Figure(1040x260)\n",
            "Figure(1500x1500)\n",
            "Figure(1040x260)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1_SesjCoe-MY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}